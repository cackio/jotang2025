# q678

## q6:异步内存拷贝相比同步拷贝有什么优势

1. 隐藏内存传输延迟

>**同步拷贝**：调用时 CPU（或 GPU）会被阻塞，直到数据从源端拷到目的端完成才能继续执行后续指令。
**异步拷贝**：发起拷贝请求后，立即返回，CPU（或 GPU Stream）可以立刻去做其它计算或发起更多传输,从而提高系统的整体吞吐量。

2. 释放 CPU/GPU 资源
>同步拷贝会让发起方空转等待，浪费宝贵的计算资源。
异步拷贝发起后即归还线程/流，能去跑其它任务，提高整体吞吐。

3. 更灵活的调度
>异步拷贝立即返回，用户可以使用事件（cuda中的cudaEvent_t） 来精确控制何时等待、何时同步，且可以跨stream或跨线程组合复杂任务依赖图，做任务调度。

## q7 一个矩阵乘法在单核心CPU上实现的的理论性能

参考Roofline Model，有几个关键的指标参数

1. 计算平台的两个指标：算力π与带宽β
>算力π：也称为计算平台的性能上限，指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是FLOP/s。
>
>带宽β：也即计算平台的带宽上限，指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是Byte/s。
>
>计算强度上限 ：两个指标相除即可得到计算平台的计算强度上限。它描述的是在这个计算平台上，单位内存交换最多用来进行多少次计算。单位是FLOPs/Byte。

2. matmul的两个指标：计算量与访存量
>计算量：对A(M×K)与B(K×N)做C=A×B，最朴素三重循环需FLOPstotal​=2×M×N×K
>访存量：D = 4(MK+KN+MN)Bytes
>算术强度：I = 2MNK/(4(MK+KN+MN)) (FLOAT/Bytes)

根据Roof-line Model：
当I小于平台的计算强度上限时，matmul的理论性能P = β*I
当I大于等于平台的计算强度上限时，matmul的理论性能P = π

假设单核CPU使用AVX2指令并行8个float，每个周期发射两条FMA，主频为2.6Hz（假设不会降低）
π​ = 2(FMA/cycle)×8(float/FMA)×2.6×1e9(cycle/s)=41.6GFLOAT/

假设带宽β为60 GB/s，平台的计算强度上限Imax = (π​)/β = 0.69 FLOAT/Bytes

I < 0.69 → 带宽 bound（性能被内存拉低）
I > 0.69 → 计算 bound（性能逼近算力上限）

若M=N=K=64，则I = 10.67>0.69，则此时matmul的理论性能P = π。


## q8 使用多线程在CPU上实现矩阵乘法

沿用q3的向量化方法，按行分块分给各个线程，具体代码如下：
```c
inline void matmul_avx2_row(const float *restrict rowA,
                                   const float *restrict B,
                                   float *restrict       rowC,
                                   size_t N, size_t K)
{
    const size_t simd_w = 8;
    size_t j;
    for (j = 0; j + simd_w <= N; j += simd_w) {
        __m256 vsum = _mm256_setzero_ps();
        for (size_t k = 0; k < K; k++) {
            __m256 va = _mm256_set1_ps(rowA[k]);
            __m256 vb = _mm256_loadu_ps(&B[k*N + j]);
            vsum = _mm256_fmadd_ps(va, vb, vsum);
        }
        _mm256_storeu_ps(&rowC[j], vsum);
    }
}

//按行分块
void matmul_avx2_mt(const float *restrict A,
                    const float *restrict B,
                    float *restrict       C,
                    size_t M, size_t N, size_t K)
{

    #pragma omp parallel for schedule(static)
    for (size_t i = 0; i < M; i++) {
        const float *rowA = A + i*K;
        float       *rowC = C + i*N;
        matmul_avx2_row(rowA, B, rowC, N, K);
    }
}
```
得到性能数据：matmul: 0.539941 ms，比q3中的单线程实现又要快不少。

1. 任务划分
>* 行划分：最简单也最高效的做法是对外层的 i（行索引）做并行，每个线程负责一段连续的行区间
>* 静态调度：#pragma omp parallel for schedule(static) 默认将 M 行均匀分给 num_threads，并保证每个线程拿到自己的一段连续行，进一步提高缓存局部性。

2. 缓存与内存通信优化
> 按行分块，读写分离，最小化通信
>* C 写入：线程各自写自己负责的行，无冲突；
>* A 读取：同样按行划分，每个线程只读自己行对应的 A，可复用 L1/L2；
>* B 读取：所有线程都访问 B，但访问模式为“按列分块后连读8元素”，能充分利用缓存行；
>无需额外的锁或原子操作。

