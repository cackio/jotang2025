# q3

先看测试结果
```
clang -O3 -march=native  test_compiler.c -o matmul_clang
matmul: 11.924942 ms

gcc -O3 -march=native test_compiler.c -o matmul_gcc
matmul: 1.648759 ms
```
可以看到性能差了大约10倍。

编译命令行中有`-march=native`，这是GCC/Clang提供的一个编译选项，用来告诉编译器将目标架构设为本机CPU，这能让编译器使用所有本机支持的指令集扩展，这就使得编译器能进行SIMD优化。

clang汇编代码
```
matmul_clang.s
matmul的最内层循环
.LBB2_8:                                #   Parent Loop BB2_5 Depth=1
                                        #     Parent Loop BB2_6 Depth=2
                                        #       Parent Loop BB2_7 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovss	1024(%rsi,%rbx), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm1, %xmm0    # xmm0 = (xmm1 * mem) + xmm0
	addq	$1024, %rcx                     # imm = 0x400
	addq	$4, %rbx
	jne	.LBB2_8
```
vmovss：标量加载一个单精度浮点。
vfmadd231ss：标量融合乘加，等价于 xmm0 += xmm1 * [rcx]。
指针增量 rcx+=1024 用来从 B 的一行跳到下一行相同列，rbx+=4 用来遍历 A 的连续元素。

可以从汇编代码看出一次循环只进行了一个乘累（FLOP），并且对B矩阵元素是按列访问的，每次都会跨1024字节，就会造成频繁的cache miss，所以会比gcc编译的程序慢很多。

gcc汇编代码
```
matmul_gcc.s
matmul的最内层循环
.L29:
	vbroadcastss	(%rdi), %ymm1
	addq	$1024, %rsi
	addq	$4, %rdi
	vfmadd231ps	-1024(%rsi), %ymm1, %ymm0
	cmpq	%rsi, %r9
	jne	.L29
```
vbroadcastss：将 A[i][k]（rdi 指向）广播到ymm1的所有8×float lanes
vfmadd231ps：针对 8 个并行单精度浮点执行融合乘加，极大提高每次迭代的吞吐。

可以看到gcc向量化做得更好一点，充分利用AVX向量单元，一次完成 8 次乘累，显著提升利用率，对于B矩阵会每次按行读取8个float，大幅提升了内存局部性，所以编译出的算子性能大约十倍于clang

那不让编译器使用指令集扩展呢？给出性能测试数据：
```
clang -O3 test_compiler.c -o matmul_clang
matmul: 8.214324 ms

gcc -O3 test_compiler.c -o matmul_gcc
matmul: 2.218961 ms
```
可以看到clang编译的程序比先前更快了，而gcc有些慢了。

不使用-march=native的汇编代码：

**clang**
```
matmul_clang1.s
matmul的最内层循环
.LBB2_104:                              #   Parent Loop BB2_101 Depth=1
                                        #     Parent Loop BB2_102 Depth=2
                                        #       Parent Loop BB2_103 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movss	-4(%rsi,%rbp,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	mulss	-1024(%rcx), %xmm1
	addss	%xmm0, %xmm1
	movss	(%rsi,%rbp,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	mulss	(%rcx), %xmm0
	addss	%xmm1, %xmm0
	addq	$2, %rbp
	addq	$2048, %rcx                     # imm = 0x800
	cmpq	$256, %rbp                      # imm = 0x100
	jne	.LBB2_104
```
这段代码的内层循环每次处理两对标量乘累（A[i][k-1]×B[k-1][j] + A[i][k]×B[k][j]），利用SSE的 movss/mulss/addss 指令，但仍是标量操作，只是展开了两次以减少分支和索引开销，但比先前的单标量操作要好一些，所以性能会好一点。

**gcc**
```
matmul_gcc1.s
matmul的最内层循环
.L28:
	movss	(%rdi), %xmm1
	movups	(%rsi), %xmm2
	addq	$1024, %rsi
	addq	$4, %rdi
	shufps	$0, %xmm1, %xmm1
	mulps	%xmm2, %xmm1
	addps	%xmm1, %xmm0
	cmpq	%rsi, %r9
	jne	.L28
```
这段代码利用SSE向量指令一次处理4次乘累，比起AVX指令的8次乘类差了一点，所以性能会稍慢。

从以上可以看出，naive matmul的性能和编译器的优化手段息息相关，在本例中clang没有实现向量化，且B矩阵每次都跨行读取，导致了大量cache miss;gcc实现了向量化，虽然B矩阵也是跨行访问，但一次会访问8/4个float，更好地利用了缓存。这些优化是由编译器决定的，不同编译器在优化策略上差异明显，有的激进，有的保守，既然如此，那可以写一个自己写一个向量化的matmul算子，看看性能是否差很多。
```c
void matmul_avx2(const float *restrict A,
                 const float *restrict B,
                 float *restrict       C,
                 size_t M, size_t N, size_t K)
{
    const size_t simd_w = 8;                  //一次并行 8 个 float
    size_t j;

    for (size_t i = 0; i < M; i++) {
        const float *rowA = A + i*K;
        float *rowC = C + i*N;

        for (j = 0; j + simd_w <= N; j += simd_w) {
            __m256 vsum = _mm256_setzero_ps();  

            for (size_t k = 0; k < K; k++) {
                __m256 va = _mm256_set1_ps(rowA[k]);             
                __m256 vb = _mm256_loadu_ps(&B[k*N + j]);         
                vsum = _mm256_fmadd_ps(va, vb, vsum);
            }
            //存回C
            _mm256_storeu_ps(&rowC[j], vsum);
        }
    }
}
```
性能测试数据：
```
clang -O3 -march=native  test_compiler.c -o matmul_clang
matmul: 1.767841 ms

gcc -O3 -march=native test_compiler.c -o matmul_gcc
matmul: 1.792874 ms
```

可以看到，两者的性能相差无几。

相关代码文件在同一文件夹下。