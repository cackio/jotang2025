# q1 将 llama 3b 模型导出为onnx格式并用 onnx runtime 进行部署

我把llama 3B模型下载到了本地，结果在导出为onnx格式时程序被kill掉了，该模型对我的PC来说太大，会cuda out of memory，cpu和cuda都试过了，都没能成功，然后试了好几种办法，结果都不行，只得到了一个不完整的llama-hf-3b-onnx，部署推理时用不了。所以我最后用的是tiny-gpt2，用这个模型导出到部署推理都没啥问题，就是得到的推理结果肯定远远不如其他模型。


